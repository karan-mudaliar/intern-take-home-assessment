{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandasql as psql\n",
    "import sqlite3\n",
    "import re\n",
    "import numpy as np\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = '../data'\n",
    "bronze_path = '../Submission/bronze_data/transaction_data'\n",
    "silver_path = '../Submission/silver_data/transaction_data'\n",
    "\n",
    "bronze_holiday_path = '../Submission/bronze_data/holiday_data'\n",
    "bronze_weather_path = '../Submission/bronze_data/weather_data'\n",
    "bronze_location_path = '../Submission/bronze_data/location_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_schema = {\n",
    "                        'location_id': str, \n",
    "                        'transaction_id': str, \n",
    "                        'profit': float\n",
    "                    }\n",
    "\n",
    "location_schema = {\n",
    "                    \t'location_id' : str,\n",
    "                        'population': int,\n",
    "                        'elevation': float\n",
    "                }\n",
    "\n",
    "weather_schema = {\n",
    "                    'location_id': str,\n",
    "                    'date': str,\n",
    "                    'temperature': float,\n",
    "                    'pressure': float,\n",
    "                    'humidity': float,\n",
    "                    'cloudy': bool,\n",
    "                    'precipitation': bool\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_profit(value):\n",
    "    try:\n",
    "        if '$' in str(value):\n",
    "            value = value.replace('$', '')\n",
    "        if '-' in str(value):\n",
    "            parts = str(value).split('-')\n",
    "            clean_value = parts[-1]\n",
    "        else:\n",
    "            clean_value = value\n",
    "\n",
    "        return float(clean_value)\n",
    "    except ValueError:\n",
    "        # Return np.nan in case of conversion failure\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invalid_location_check(df, column_name = 'location_id'):\n",
    "\n",
    "    invalid_rows = df[df[column_name].str.match(r'^\\d{3}$') == False]\n",
    "    \n",
    "    return invalid_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invalid_date_check(df, pattern, column_name = 'date'):\n",
    "\n",
    "    def check_format(value):\n",
    "        return not bool(re.match(pattern, str(value)))\n",
    "\n",
    "    invalid_rows = df[df[column_name].apply(check_format)]\n",
    "    \n",
    "    return invalid_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invalid_transaction_id_check(df,column_name = 'transaction_id'):\n",
    "    \n",
    "    def contains_non_numeric(value):\n",
    "        return not value.isdigit()\n",
    "    \n",
    "    non_numeric_rows = df[df[column_name].astype(str).apply(contains_non_numeric)]\n",
    "    return non_numeric_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invalid_profit(df,column_name = 'profit'):\n",
    "    \n",
    "    df[column_name] = df[column_name].replace(\"nan\", np.nan)\n",
    "    numeric_values = pd.to_numeric(df[column_name], errors='coerce')\n",
    "    non_float_rows = df[numeric_values.isna()]\n",
    "\n",
    "    return non_float_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_path_bronze(df, file_path, file_name):\n",
    "    \n",
    "    full_file_path = f\"{file_path}/{file_name}\"\n",
    "    df.to_csv(full_file_path, index=False)\n",
    "\n",
    "    return print(f\"DataFrame saved to {full_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_transactions_json(path):\n",
    "\n",
    "    json_files = glob.glob(os.path.join(path, 'transactions*.json'))\n",
    "    \n",
    "    schema = {\n",
    "        'location_id': str, \n",
    "        'date': str, \n",
    "        'transaction_id': str, \n",
    "        'profit': str\n",
    "    }\n",
    "    \n",
    "    df_list = []\n",
    "    df_counts = []\n",
    "\n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            temp_df = pd.read_json(json_file, dtype=schema)\n",
    "            df_list.append(temp_df)\n",
    "            df_counts.append(len(temp_df))\n",
    "        except ValueError as e:\n",
    "            print(f\"Error reading {json_file}: {e}\")\n",
    "\n",
    "    if not df_list:  # If df_list is empty after the loop\n",
    "        print(\"No valid JSON files found or all files failed to load.\")\n",
    "        return None, None\n",
    "\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    return combined_df, df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_transactions_txt(path):\n",
    "    txt_files = glob.glob(os.path.join(path, 'transactions*.txt'))\n",
    "    \n",
    "    df_list = []\n",
    "    df_counts = []\n",
    "    schema = {\n",
    "        'location_id': str, \n",
    "        'date': str, \n",
    "        'transaction_id': str, \n",
    "        'profit': str\n",
    "    }\n",
    "    \n",
    "\n",
    "    for txt_file in txt_files:\n",
    "        try:\n",
    "            temp_df = pd.read_csv(txt_file, delimiter='\\t', dtype=schema)\n",
    "            df_list.append(temp_df)\n",
    "            df_counts.append(len(temp_df))\n",
    "        except Exception as e:  # Using a broad catch to handle any error during file reading\n",
    "            print(f\"Error reading {txt_file}: {e}\")\n",
    "\n",
    "    if not df_list:  # If df_list is empty after the loop\n",
    "        print(\"No valid TXT files found or all files failed to load.\")\n",
    "        return None, None\n",
    "\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    return combined_df, df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_transactions_csv(path):\n",
    "    csv_files = glob.glob(os.path.join(path, 'transactions*.csv'))\n",
    "    df_list = []\n",
    "    df_counts = []\n",
    "\n",
    "    schema = {\n",
    "        'location_id': str, \n",
    "        'date': str, \n",
    "        'transaction_id': str, \n",
    "        'profit': str\n",
    "    }\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            # Read the CSV file. Pandas assumes comma as the default delimiter for CSV.\n",
    "            temp_df = pd.read_csv(csv_file, dtype=schema)\n",
    "            df_list.append(temp_df)\n",
    "            df_counts.append(len(temp_df))\n",
    "        except Exception as e:  # Using a broad catch to handle any error during file reading\n",
    "            print(f\"Error reading {csv_file}: {e}\")\n",
    "\n",
    "    if not df_list:  # If df_list is empty after the loop\n",
    "        print(\"No valid CSV files found or all files failed to load.\")\n",
    "        return None, None\n",
    "\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    return combined_df, df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bool(df, column_name = 'holiday'):\n",
    "    failed_columns = []\n",
    "    \n",
    "    for column_name in df.columns:\n",
    "        # Drop rows where the value in the current column is NaN or None and keep the index of dropped rows\n",
    "        non_missing_values = df[column_name].dropna()\n",
    "        dropped_indices = df[df[column_name].isna()].index.tolist()\n",
    "\n",
    "        # Check if all non-missing values in the column are strictly True or False\n",
    "        if not all(non_missing_values.apply(lambda x: type(x) == bool)):\n",
    "            # If the column fails the check, add it and the dropped row indices to the failed_columns list\n",
    "            failed_columns.append({'Column': column_name, 'Dropped Row Indices': dropped_indices})\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    failed_columns_df = pd.DataFrame(failed_columns)\n",
    "\n",
    "    return failed_columns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
